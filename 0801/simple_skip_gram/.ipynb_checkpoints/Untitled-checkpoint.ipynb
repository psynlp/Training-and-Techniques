{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V\n",
    "- 기존 통계 기법의 단점을 커버하는 추론기법\n",
    "- 맥락 정보를 입력받아 각 단어의 출현 확률을 구함\n",
    "- 학습 결과로 얻어진 `가중치`를 `단어의 분산 표현`으로 이용\n",
    "\n",
    "![process](img/process.png)\n",
    "\n",
    "|특징| 통계 기법| 추론기법 |\n",
    "|:--------|:--------|:--------|\n",
    "|학습| 말뭉치 전체 통계로 1회 학습 | 말뭉치의 일부를 여러번 학습 |\n",
    "|새로운 단어 추가| 처음부터 다시 계산 | 추가 학습 가능 |\n",
    "|정밀도| 단어의 유사성 |유사성 + 단어 사이의 패턴|\n",
    "\n",
    "## CBOW vs Skip-gram\n",
    "- 주변 맥락을 통해 단어를 예측 vs 단어를 통해 주변 맥락을 예측\n",
    "- Skip-gram이 학습에 더 오랜 시간이 걸리지만 성능이 더 좋음\n",
    "\n",
    "![cbow_vs_skipgram](img/cbow_vs_skipgram.png)\n",
    "\n",
    "### CBOW\n",
    "![cbow](img/cbow.png)\n",
    "\n",
    "\n",
    "### Skip-gram\n",
    "![cbow](img/skipgram.png)\n",
    "\n",
    "### W2V의 결과?\n",
    "- 맥락을 통해 나올 단어를 예측\n",
    "- $W_{in}$, $W_{out}$ 두 종류의 Weight 존재\n",
    "- $W_{in}$는 각 행이 각 단어의 분산 표현\n",
    "- $W_{out}$는 각 열이 각 단어의 분산 표현\n",
    "- 보통 $W_{in}$이 단어의 분산 표현을 잘 나타냄\n",
    "\n",
    "![weight](img/weight.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구현\n",
    "- 순전파, 역전파를 위해 정형화된 클래스를 이용\n",
    "\n",
    "### 입력층 - 은닉층\n",
    "- 단어를 적절한 고정된 입력으로 변환해야 학습이 가능\n",
    "- One-hot vector 사용\n",
    "- 은닉층의 차원이 전체 단어의 차원보다 작아야 차원을 감소하며 단어의 분산을 얻음\n",
    "\n",
    "![input1](img/input1.png)\n",
    "\n",
    "![inpit3](img/input3.png)\n",
    "\n",
    "![inpit4](img/input4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 각 단어의 원핫 벡터\n",
      "[1 0 0 0 0 0 0] \n",
      "\n",
      "W: 모든 단어를 나타내는 가중치\n",
      "[[-0.75205386 -1.53891402 -0.12257136]\n",
      " [ 2.21352213 -0.77153767  2.34023862]\n",
      " [-1.08018893  0.38376832 -0.51920174]\n",
      " [-0.09751996  0.26537902 -1.21223167]\n",
      " [ 0.54591482  0.17679802 -0.90072075]\n",
      " [-1.58061226  0.30573001 -0.73536499]\n",
      " [-0.41475626  1.79854346 -0.3758167 ]] \n",
      "\n",
      "h: 은닉층 결과(특정 단어의 가중치 추출)\n",
      "[-0.75205386 -1.53891402 -0.12257136]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([1, 0, 0, 0, 0, 0, 0])\n",
    "W = np.random.randn(7, 3)\n",
    "h = np.matmul(c, W)\n",
    "\n",
    "print(\"c: 각 단어의 원핫 벡터\")\n",
    "print(c, \"\\n\")\n",
    "\n",
    "print(\"W: 모든 단어를 나타내는 가중치\")\n",
    "print(W, \"\\n\")\n",
    "\n",
    "print(\"h: 은닉층 결과(특정 단어의 가중치 추출)\")\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatMul 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]  # 파라미터가 여러개일 수 있어서 리스트 사용\n",
    "        self.grads = [np.zeros_like(W)]  # 역전파된 결과를 저장하기 위해 사용\n",
    "        self.x = None  # 역전파시 필요\n",
    "\n",
    "    # 순전파\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    # 역전파\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW  # mutable한 데이터이기 때문에 같은 메모리를 가리키지 않도록 함\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatMul 확인\n",
    "![cbow](img/skipgram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58107258 -0.11122237  0.37298207]]\n"
     ]
    }
   ],
   "source": [
    "# 샘플 맥락 데이터\n",
    "c = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "# 가중치 초기화\n",
    "W_in = np.random.randn(7, 3)\n",
    "\n",
    "# 계층 생성\n",
    "in_layer = MatMul(W_in)\n",
    "\n",
    "# 순전파\n",
    "h = in_layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 은닉층 - 출력층\n",
    "- 은닉층 - 출력층 사이의 $W_{out}$은 $W_{in}$를 전치한 shape을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.42881164 -1.65270976 -1.99194813 -1.37293306  0.23078604  0.35747227\n",
      "  -0.47239193]]\n"
     ]
    }
   ],
   "source": [
    "# 샘플 맥락 데이터\n",
    "c = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "# 가중치 초기화\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "# 계층 생성\n",
    "in_layer = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 순전파\n",
    "h = in_layer.forward(c)\n",
    "s = out_layer.forward(h)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력층 - 확률(Softmax)\n",
    "- 출력 결과를 확률로 만들기 위해 Softmax를 이용\n",
    "- 오차를 구하기 위해 cross-entropy를 이용하며, 보통 합쳐서 Softmax with Loss 계층 사용\n",
    "- 추론시에는 Softmax만 사용해야 함(주의)\n",
    "- 오차는 출력과 정답의 차이\n",
    "\n",
    "![softmax_with_loss](img/softmax_with_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블\n",
    "\n",
    "    # 소프트맥스\n",
    "    def softmax(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x - x.max(axis=1, keepdims=True)\n",
    "            x = np.exp(x)\n",
    "            x /= x.sum(axis=1, keepdims=True)\n",
    "        elif x.ndim == 1:\n",
    "            x = x - np.max(x)\n",
    "            x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    # 크로스 엔트로피\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "\n",
    "        # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1)\n",
    "\n",
    "        batch_size = y.shape[0]\n",
    "\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(x)\n",
    "\n",
    "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer1 = SoftmaxWithLoss()\n",
    "        self.loss_layer2 = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "        s = self.out_layer.forward(h)\n",
    "        l1 = self.loss_layer1.forward(s, contexts[:, 0])\n",
    "        l2 = self.loss_layer2.forward(s, contexts[:, 1])\n",
    "        loss = l1 + l2\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dl1 = self.loss_layer1.backward(dout)\n",
    "        dl2 = self.loss_layer2.backward(dout)\n",
    "        ds = dl1 + dl2\n",
    "        dh = self.out_layer.backward(ds)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
