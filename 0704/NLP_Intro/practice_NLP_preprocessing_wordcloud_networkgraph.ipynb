{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with Python Workshop\n",
    "\n",
    "## YK Lee \n",
    "### 2019.07.04 Thur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: \n",
    "\n",
    "Stanford NLP: https://nlp.stanford.edu/ <br>\n",
    "Text Mining : https://ko.coursera.org/learn/text-mining\n",
    "\n",
    "그 외 링크 별도 기재 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 텍스트 분석의 단계 \n",
    "\n",
    "#### (1) Data Collection => (2)Text Normalization => (3) Weight => (4) Transform => (5) Classification/Cluster\n",
    "\n",
    "#### (1) Data Collection에는 manual, web scraping 등이 포함\n",
    "\n",
    "#### (2) Text Normalization은 다음 단계로 나뉜다. \n",
    "\n",
    "    Tokenization => Stopwords Removal => Stemmization/Lemmatization => Pos-Tagging => Key-word Extraction\n",
    "\n",
    "#### (3) Weight에는 Frequency, Term weight의 계산이 있다. \n",
    "\n",
    "#### (4) Transform 에는 SVD, 등의 주어진 키워드에 근거하여 문서들을 할당하는 분류기법이 있다 (Decision Tree, NN, Bayes, PCA, SVM, LSI, LDA..etc.)\n",
    "\n",
    "#### (5) Classification/Clustering에는 사전 정보 없이 비슷한 문서를 집단으로 묶는 K-means 같은 군집 기법이 있다. \n",
    "\n",
    "이번 시간에는 이후 모델링 단계로 가기 전단계인 텍스트 전처리에 대해 알아보자. 텍스트 전처리가 모든 텍스트 분석 과정의 80%를 차지할 만큼 중요하면서도 가장 시간이 많이 소요되는 단계이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 텍스트 단위\n",
    "\n",
    "텍스트 단위로는 주로 문자 (글자), 단어, 문장 단위가 있다. \n",
    "\n",
    "이 중, 단어가 의미를 담고 있을 확률이 가장 많기 때문에 텍스트 분석에서 가장 많이 쓰이는 단위이며 기계 학습 모델도 단어 기반으로 많이 개발되고 있다. \n",
    "\n",
    "여기서 '단어'에 해당하는 용어는 아래와 같이 나뉠 수 있다. \n",
    "\n",
    "   **word**: 구분된 문자열 <br>\n",
    "   **term**: normalized된 단어 (대소문자 통일, 형태 복원, 철자 교정 등), word와 동일하게 쓰이기도 한다. <br>\n",
    "   **token**: 유용한 의미적 단위로 함게 모여지는 일련의 문자열, 구분 기호 (띄어쓰기 등) 사이의 글자 시퀀스. 텍스트 분석의 최소 단위 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**토큰** (Token) 은 띄어쓰기 등의 구문 기호로 구분되는 문자열의 최소 단위이다. 의미를 담고 있는 단어일 수도 있고 아닐 수도 있다. 의미를 담고있느냐 아니냐의 여부와는 상관없이 하나의 단어 조각이라고 생각하면 된다. \n",
    "\n",
    "**토큰화**(Tokenization)는 문헌 단위의 문자열이 주어졌을때, 토큰으로 쪼개는 방법 또는 그 과정을 일컫는다. 키워드 추출시 분석에 불필요한 작업을 일컬으기도 하나 이는 보통 불용어 처리라고 분류하기도 한다. \n",
    "\n",
    "영어의 경우 단어에 조사가 붙지 않기때문에 한글보다 토큰화가 더 잘되는 편이다. \n",
    "\n",
    "예시. 2.1.1. 영어 토큰화\n",
    "\n",
    "\"Hello, my name is Yoon\" => \"Hello\"/\",\"/\"my\"/\"name\"/\"is\"/\"Yoon\"\n",
    "\n",
    "예시 2.1.2. 한국어 토큰화\n",
    "\n",
    "\"안녕, 내 이름은 윤이야\" => \"안녕\"/\",\"/\"내\"/\"이름\"/\"은\"/\"윤\"/\"이\"/\"야\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.  토큰화1 : split() with python basic libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘Women’s Rights Are Human Rights’ by Hillary Clinton(1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc = \"\"\"If there is one message that echoes forth from this conference, let it be that human rights are women’s rights and women’s rights are human rights once and for all. Let us not forget that among those rights are the right to speak freely — and the right to be heard.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_tokens = hc.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. 토큰화2 : import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk 라이브러리에는 토큰화를 포함하여 기본적인 전처리에 필요한 다른 함수들도 제공한다. 먼저 토큰화를 위한 라이브러리를 살펴보자. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화할 문구는 마찬가지로 위 2.1번에서 했던 hilary clinton의 연설문이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hc) #check again, just in case.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk에서 제공하는 토큰화 함수는 word_tokenize이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk의 ```WordPunctTokenizer```는 구두점도 따로 분리해준다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bases: ``` nltk.tokenize.regexp.RegexpTokenizer ```\n",
    "\n",
    "Tokenize a text into a sequence of alphabetic and non-alphabetic characters, using the regexp \\w+|[^\\w\\s]+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = nltk.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"\n",
    "Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet용 토크나이저 ```TweetTokenizer```도 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tknzr.tokenize(s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 불용어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 문장을 이해할 때 굳이 필요 없는 정보 (구두점, 주어, 문장 부호 등)이거나, 도메인에 따라 중요도가 낮아지는 단어들을 뜻한다. \n",
    "\n",
    "영어의 대표적인 불용어는 ``` the, a, um, you, omg, oh, literally, I .. etc. ``` 등이 있다. \n",
    "\n",
    "한국어의 대표적인 불용어는 ``` 없, 있, 그, 저, 이 좀, 한.. ``` 등이 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- 주의 \n",
    "\n",
    "하나의 단어, 인용구, 신조어일 경우, 토큰화를 띄어쓰기 단위로만 끊으면 다소 문제가 될 수 있다.  \n",
    "\n",
    "1)  등과 같이 값, 가치를 표현하는 텍스트가 있을 경우: ``` ' . , ``` 등의 구둣점, 쉼표가 필요할 경우이다. \n",
    "    \n",
    "    $24.95, 1,000,000.00 \n",
    "    \n",
    "    \n",
    "2) 이름/고유명칭\n",
    "\n",
    "\"John Smith\" => \"John\"/\"Smith\"/ 이름이 John이고 성이 Smith => John과 Smith ?\n",
    "\n",
    "3) 축약 \n",
    "\n",
    "\"It's fine.\" => \"It is fine\" \"I'm Korean\" => \"I am Korean\"\n",
    "\n",
    "별도의 단어 사전을 통해 필터링해두거나 규칙을 정한 후 (예: I'm => I am) 분리 또는 결합 등을 통해 추가 전처리가 필요함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. nltk로 불용어 제거 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk의 ```stopwords()```함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어 기본 불용어 (기본 제공)\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "hc_tokens = word_tokenize(hc)\n",
    "\n",
    "hc_filter = [word for word in hc_tokens if word not in stop_words]\n",
    "\n",
    "print(\"First, tokeinze: \\n\",hc_tokens)\n",
    "\n",
    "print(\" ------ \")\n",
    "\n",
    "print(\"Next, remove stopwords: \\n\",hc_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hc_tokens)<len(hc_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 불용어 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.update(\".\", \"-\", \"s\",\", \", \"us\",\"the\",\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_filter_update = [word for word in hc_tokens if word not in stop_words]\n",
    "print(\"This is the original version:\\n \\n\", hc_tokens)\n",
    "print(\"\\n\\nThis is the updated version:\\n \\n\", hc_filter_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. normalization (정규화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어학적 정의로는 서로 다르게 표현되었으나 의미는 같은 단어들을 통이해주는 작업 (대소문자 통합, 어근 추출 등)을 일컫는다.\n",
    "\n",
    "- 대소문자 통합\n",
    "- 어근 추출 (lemmatization, stemming)\n",
    "\n",
    "이 외에도 축약형 확장 (expansion) 등이 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. stemming\n",
    "\n",
    "어근(stem)만 남겨두고 나머지는 싹뚝 잘라버린다. \n",
    "\n",
    "PorterStemmer (포터 어간 추출 알고리즘)이 영어 전처리 과정에 많이 쓰이는 대표적인 알고리즘이다. http://www.tartarus.org/~martin/PorterStemmer/\n",
    "\n",
    "- stemming의 과정\n",
    "\n",
    "<img src = \"https://slideplayer.com/slide/4698162/15/images/5/Porter+s+Stemmer+Porter+Stemming+Algorithm+Complex+suffixes.jpg\" width = 500>\n",
    "\n",
    "\n",
    "한국어는 stemming이라는 컨셉이 완벽히 적합하진 않고, **형태소 분석** 과정에서 이루어진다.\n",
    "\n",
    "한국어 형태소 분석은 *KoNLPy* [link](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)와 *soynlp*가 있다. (이후 단계에 다시 방문)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. lemmatization\n",
    "\n",
    "한 단어가 여러가지 형식으로 표현되어 있는 것을 하나의 단일 형식으로 묶어주는 기법이며, Stanford의 Manning과 Schutze (1999)에 의해 소개되었다. \n",
    "\n",
    "자연어 처리 기법의 근간이 되는 *형태소 분석* 및 *복원*을 해주기 때문에 더 정확한 단어 수준의 분석을 가능하게 해준다. \n",
    "\n",
    "- 형태소 종류: <br>\n",
    " -- 어간(stem), 접사(affix)\n",
    "\n",
    "<br>\n",
    "- 형태소 분석 종류: \n",
    " -- 굴절 형태론(inflectional morphology, \"cutting\" <=> \"cut\"), 파생 형태론 (derivational morphology, \"destruction\" <=> \"destroy\")\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "- 예시: \n",
    "\n",
    "am, are, is => \"be\"로 복원\n",
    "\n",
    "(참고: https://ratsgo.github.io/natural%20language%20processing/2017/03/22/lexicon/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "words = ['died', 'watching', 'had']\n",
    "[lemma.lemmatize(w, 'v') for w in words] # 이들 단어가 동사로 쓰였다는 사실을 알려주기 위해 'v' 옵션추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. POS tagging\n",
    "\n",
    "품사 태깅 (Part-of-speech(POS) tags/tagging)\n",
    "문장에서 각각의 단어에 해당되는 품사를 레이블로 달아주는 것 (=이름표 달기)\n",
    "\n",
    "각각의 단어가 어떤 품사로 쓰였는지에 따라 토큰화, 의도 파악이 달라질 수 있기에 텍스트 분석에서 중요하고 필수적인 과정\n",
    "\n",
    "2.4.1. 규칙 기반\n",
    "사전을 사용해서 각각의 단어에 그 단어가 가질 수 있는 품사 리스트를 부여.\n",
    "수작업으로 만든 대량의 규칙을 이용해서 품사의 리스트에서 해당 단어에 맞는 하나의 품사를 선택. 한글 같은 경우 세종 말뭉치가 있음. 세종 말뭉치에는 이런 품사들을 품사별로 사전을 구축한 것이 있음.\n",
    "\n",
    "<br>\n",
    "\n",
    "2.4.2. 기계 학습 기반\n",
    "각각의 단어의 품사 결정을 해당 단어의 학습 데이터에서 가장 일반적인 품사로 함.\n",
    "히든 마르코프 모델 (hidden markov model: hmm) or conditional random field라는 분류기를 사용. 모든 가능한 태그 순열 중에서 문헌 집단 내에 주어진 단어들의 순열 (observed sequence)에서 가장 가능성이 있는 태그열을 찾음.\n",
    "\n",
    "\n",
    "### Alphabetical list of part-of-speech tags used in the Penn Treebank Project:\n",
    "https://www.cis.upenn.edu/~treebank/\n",
    "\n",
    "```\n",
    "CC Coordinating conjunction\n",
    "CD Cardinal number\n",
    "DT Determiner\n",
    "EX Existential there\n",
    "FW Foreign word\n",
    "IN Preposition or subordinating conjunction\n",
    "JJ Adjective\n",
    "JJR Adjective, comparative\n",
    "JJS Adjective, superlative\n",
    "LS List item marker\n",
    "MD Modal\n",
    "NN Noun, singular or mass\n",
    "NNS Noun, plural\n",
    "NNP Proper noun, singular\n",
    "NNPS Proper noun, plural\n",
    "PDT Predeterminer\n",
    "POS Possessive ending\n",
    "PRP Personal pronoun\n",
    "PRP$ Possessive pronoun\n",
    "RB Adverb\n",
    "RBR Adverb, comparative\n",
    "RBS Adverb, superlative\n",
    "RP Particle\n",
    "SYM Symbol\n",
    "TO to\n",
    "UH Interjection\n",
    "VB Verb, base form\n",
    "VBD Verb, past tense\n",
    "VBG Verb, gerund or present participle\n",
    "VBN Verb, past participle\n",
    "VBP Verb, non­3rd person singular present\n",
    "VBZ Verb, 3rd person singular present\n",
    "WDT Wh­determiner\n",
    "WP Wh­pronoun\n",
    "WP$ Possessive wh­pronoun\n",
    "WRB Wh­adverb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(hc_tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text 설명: \n",
    "\n",
    "<img src = \"https://i.ebayimg.com/images/g/B08AAOSwQ59ZYo3w/s-l300.jpg\" width = 200>\n",
    "\n",
    " 뇌 과학자 Oliver Sacks의 The man who mistook his wife for a hat에서 발췌한 내용이다. 위에서 다룬 토큰화, 불용어 제거, 스테밍을 해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "Dr P. was a musician of distinction, well-known for many years as a singer, and then, at the local School of Music, as a teacher. \n",
    "It was here, in relation to his students, that certain strange problems were first observed. \n",
    "Sometimes a student would present himself, and Dr P. would not recognize him; or, specifically, would not recognize his face. \n",
    "The moment the student spoke, he would be recognized by his voice. \n",
    "Such incidents multiplied, causing embarrassment, perplexity, fear—and, sometimes, comedy.\n",
    "For not only did Dr P. increasingly fail to see faces, but he saw faces when there were no faces to see: genially, Magoo-like, when in the street he might pat the heads of water hydrants and parking meters,taking these to be the heads of children; he would amiably address carved knobs on the furniture and be astounded when they did not reply. \n",
    "At first these odd mistakes were laughed off as jokes, not least by Dr P. himself.\n",
    "Had he not always had a quirky sense of humor and been given to Zen-like paradoxes and jests? \n",
    "His musical powers were as dazzling as ever; he did not feel ill—he had never felt better; and the mistakes were so ludicrous—and so ingenious—that they could hardly be serious or betoken anything serious. \n",
    "The notion of there being ‘something the matter’ did not emerge until some three years later, when diabetes developed. \n",
    "Well aware that diabetes could affect his eyes, Dr P. consulted an ophthalmologist, who took a careful history and examined his eyes closely. \n",
    "‘There’s nothing the matter with your eyes,’ the doctor concluded. \n",
    "‘But there is trouble with the visual parts of your brain. You don’t need my help, you must see a neurologist.’ And so, as a result of this referral, Dr P. came to me.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. 한국어 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 한글 형태소 분석\n",
    "\n",
    "한글 입력 문자열을 형태소열로 바꾸는 작업\n",
    "\n",
    "- 형태소 분석: \"띄어쓰기 단위의 어절인 단어(word)를 구성하는 각각의 형태소들을 인식하고 용언의 활용, 불규칙 활용이나 축약, 탈락현상이 일어난 형태소는 원형을 복원하는 과정 (강승식,『한국어 형태소 분석과 정보검색, 2002)\" \n",
    "\n",
    "- 한글 품사 부착 말뭉치: 세종 태그 세트 (http://kkma.snu.ac.kr/documents/?doc=postag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 한글 형태소 분석기 종류\n",
    "\n",
    "## 2.1. libraries\n",
    "\n",
    "- KoNLPy\n",
    "\n",
    "- soynlp\n",
    "\n",
    "## 2.2. POS taggers\n",
    "\n",
    "- 꼬꼬마 http://kkma.snu.ac.kr\n",
    "\n",
    "- 한나눔 http://semanticweb.kaist.ac.kr\n",
    "\n",
    "- Komoran http://shineware.tistory.com/entry/KOMORAN-ver-24\n",
    "\n",
    "- Okt (Twitter-Korean-text) https://github.com/twitter/twitter-korean-text\n",
    "\n",
    "- khaiii(Kakao Hangul Analyzer III) http://tech.kakao.com/2018/12/13/khaiii/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Kkma, Mecab, Hannanum\n",
    "t = Okt()\n",
    "kkma = Kkma()\n",
    "#mecab = Mecab() # mecab은 윈도우에서 사용 불가\n",
    "hannanum = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "print(komoran.morphs('코모란 형태소 분석기를 사용하면 품사별로 단어를 추출할 수 있어요.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma  # 꼬꼬마 형태소 분석기 사용\n",
    "\n",
    "kkma = Kkma()\n",
    "text = \"오늘 서울의 날씨는 추워질 전망입니다. 오후 한때 소나기가 올 예정입니다.\"\n",
    "sentences = kkma.sentences(text)\n",
    "sentences[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(okt.morphs('이것도 되나욬ㅋㅋ', norm=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoNLPy의 새로운 단어가 사전에 추가되지 않는 점을 보완하고자 개발됨. (by Hyunjoong Kim)\n",
    "\n",
    "확률 모델이 아닌, 주어진 데이터를 기반으로 단어 및 명사 추출, 토큰화, 사용자 사전 추가 및 생성 + KoNLPy 등이 가능\n",
    "\n",
    "단어 추출 기법은 cohesion 기법 사용\n",
    "\n",
    "명사는 새로운 단어가 가장 많이 생성되고 사용 빈도가 높아서 따로 추출할 수 있는 기능 추가 \n",
    "\n",
    "- I. 단어 추출 + 토큰화\n",
    "\n",
    "- II. 명사 추출\n",
    "\n",
    "- III. KoNLPy + 사용자 사전\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slides: https://www.slideshare.net/kimhyunjoonglovit/pycon2017-koreannlp <br>\n",
    "link: https://lovit.github.io/nlp/2018/04/09/three_tokenizers_soynlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : https://github.com/lovit/soynlp/blob/master/tutorials/doublespace_line_corpus_(with_noun_extraction).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 II : 전처리한 데이터로 시각화하기(워드클라우드, 네트워크 그래프)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://images-na.ssl-images-amazon.com/images/I/51wUYZ1HYmL._SY355_.jpg\" width = 200/>\n",
    "\n",
    "<img src = \"https://i.pinimg.com/564x/31/44/72/314472d69bc7423e788bcb0146d08823.jpg\" width = 300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (참고) 영어는 형태소 분석을 하지 않고 따로 분류를 한다 (코모란의 경우 SL로 태깅) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "k = Komoran()\n",
    "print(k.pos('Kpop 아이돌로 태어나 다시 환생한 Artist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "폴더명은 bts2019-map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_dir = '폴더명' #(코드)\n",
    "\n",
    "fn_list = os.listdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for fn in fn_list :\n",
    "    if 'txt' in fn :\n",
    "        fn = os.path.join(root_dir, fn)\n",
    "        with open(fn) as f :\n",
    "            doc = f.readlines()\n",
    "            docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts_sent = [] #문장을 담은 변수 \n",
    "for doc in docs :\n",
    "    for sent in doc :\n",
    "        bts_sent.append(sent.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 확인해보기 (코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_bts_sent = [k.pos(sent) for sent in bts_sent]\n",
    "#품사 태그 변수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bts_word_list = [] #형태소 분석을 위해 정의한 변수\n",
    "for sent in tagged_bts_sent:    \n",
    "    for word, tag in sent:\n",
    "        #if tag in ['NNG','NNS']:\n",
    "            bts_word_list.append(word)\n",
    "            \n",
    "\n",
    "#확인해보기 (코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "bts_counter = Counter(bts_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도 확인해보기 (코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위 코드를 수정하여 명사 토큰만 출력한 후 빈도수를 계산해보자 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 키워드(명사) 인덱스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nouns = set()\n",
    "for sent in tagged_bts_sent:    \n",
    "    for word, tag in sent:\n",
    "        if tag in ['NNP', 'NNG']:\n",
    "            unique_nouns.add(word)\n",
    "\n",
    "unique_nouns = list(unique_nouns)\n",
    "noun_index = {noun: i for i, noun in enumerate(unique_nouns)}\n",
    "noun_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어 '사랑' 인덱스 찾기\n",
    "\n",
    "noun_index[' '] #단어 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "noun_ext = LRNounExtractor_v2(verbose=True)\n",
    "\n",
    "# 사전 기반 단어(명사) 추출기\n",
    "nouns = noun_ext.train_extract(bts_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of nouns = {}'.format(len(nouns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BTS MAP THE SOUL 앨범 수록곡 중 길이가 2 이상이며, 빈도수 기준 상위 100 개인 명사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nouns = [(noun, score) for noun, score in nouns.items() if len(noun) > 1]\n",
    "selected_nouns = sorted(selected_nouns, key=lambda x:-x[1].frequency)[:100]\n",
    "\n",
    "for noun, score in selected_nouns:\n",
    "    print('{} : {}'.format(noun, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 워드클라우드를 그려보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_kr = ['나','너','대도'] #불용어 커스터마이징 해보기 (코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud#, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def displayWordCloud(data = None, backgroundcolor = 'white', width=800, height=600 ):\n",
    "    wordcloud = WordCloud(\n",
    "                        font_path = '/Library/Fonts/AppleGothic.ttf', #윈도우일경우 윈도우 폰트에 맞게 수정할 것\n",
    "                        stopwords = stopwords_kr, \n",
    "                        background_color = backgroundcolor, \n",
    "                         width = width, height = height).generate(data)\n",
    "    plt.figure(figsize = (15 , 10))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time displayWordCloud(' '.join(noun_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 불용어를 업데이트해서 다시 월드클라우드를 그려보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(코드)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보너스 \n",
    "\n",
    "## 6. 네트워크 그래프 그려보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. 공출현 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "occurs = np.zeros([len(tagged_bts_sent), len(unique_nouns)])\n",
    "np.shape(occurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(tagged_bts_sent):\n",
    "    for word, tag in sent:\n",
    "        if tag in ['NNP', 'NNG']:\n",
    "            index = noun_index[word]  # 명사가 있으면, 그 명사의 인덱스를 index에 저정\n",
    "            occurs[i][index] = 1  # 문장 i의 index 자리에 1을 채워 넣는다.\n",
    "            \n",
    "occurs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurs = occurs.T.dot(occurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx #설치 필요 \n",
    "graph = nx.Graph()\n",
    "\n",
    "for i in range(len(unique_nouns)):\n",
    "    for j in range(i + 1, len(unique_nouns)):\n",
    "        if co_occurs[i][j] > 1.5:\n",
    "            graph.add_edge(unique_nouns[i], unique_nouns[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "layout = nx.spring_layout(graph, k=.5)\n",
    "nx.draw(graph, pos=layout, with_labels=True,\n",
    "        font_size=30, font_family=\"AppleGothic\",\n",
    "        alpha=0.5, node_size=3400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
